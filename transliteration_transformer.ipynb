{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "batch_size = 512\n",
    "n_epochs=150\n",
    "test_size = 0.01 #(1% of the data, around 700 samples)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, DATA_HUB='atlasia/ATAM'):\n",
    "        # Load the dataset\n",
    "        self.data = load_dataset(DATA_HUB)['train'].to_pandas().values.tolist()\n",
    "        # Create a set of all unique characters in the source and target languages\n",
    "        self.arabizi_chars = set(''.join([d[0] for d in self.data]))\n",
    "        self.arabic_chars = set(''.join([d[1] for d in self.data]))\n",
    "        # Create a dictionary mapping each character to a unique index\n",
    "        self.char2idx_ary = {char: idx for idx, char in enumerate(self.arabizi_chars)}\n",
    "        self.char2idx_ar = {char: idx for idx, char in enumerate(self.arabic_chars)}\n",
    "        # Calculate the size of the vocabulary\n",
    "        self.vocab_size_src = len(self.char2idx_ary)\n",
    "        self.vocab_size_tgt = len(self.char2idx_ar)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        darija, darija_ar = self.data[idx]\n",
    "        input_seq = [self.char2idx_ary[char] for char in darija]\n",
    "        target_seq = [self.char2idx_ar[char] for char in darija_ar]\n",
    "        input = torch.LongTensor(input_seq)\n",
    "        target = torch.LongTensor(target_seq)\n",
    "        return input, target\n",
    "\n",
    "def collate_function(batch):\n",
    "    # Extract inputs and targets from the batch\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Concatenate all sequences in the batch to find the maximum length\n",
    "    all_sequences = inputs + targets\n",
    "    \n",
    "    # Find the maximum length of sequences in the batch\n",
    "    max_seq_length = max(len(seq) for seq in all_sequences)\n",
    "    \n",
    "    # Pad all sequences to the maximum length\n",
    "    padded_inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    padded_targets = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad sequences to max_seq_length\n",
    "    padded_inputs = torch.cat([padded_inputs, torch.zeros(padded_inputs.size(0), max_seq_length - padded_inputs.size(1), dtype=torch.long)], dim=1)\n",
    "    padded_targets = torch.cat([padded_targets, torch.zeros(padded_targets.size(0), max_seq_length - padded_targets.size(1), dtype=torch.long)], dim=1)\n",
    "    \n",
    "    return padded_inputs, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransliterationDataset()\n",
    "train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The training dataset has {len(train_data)} samples.')\n",
    "print(f'The validation dataset has {len(val_data)} samples.')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationModel(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, d_model=128, nhead=2, num_encoder_layers=2, num_decoder_layers=2):\n",
    "        super(TransliterationModel, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(vocab_size_src, d_model)\n",
    "        self.embedding_tgt = nn.Embedding(vocab_size_tgt, d_model)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size_tgt)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding_src(src)\n",
    "        tgt = self.embedding_tgt(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransliterationModel(vocab_size_src=dataset.vocab_size_src, vocab_size_tgt=dataset.vocab_size_tgt).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Ensure that inputs and targets have the same length after padding\n",
    "            max_seq_length = max(inputs.size(1), targets.size(1))\n",
    "            inputs = torch.cat([inputs, torch.zeros(inputs.size(0), max_seq_length - inputs.size(1), dtype=torch.long)], dim=1)\n",
    "            targets = torch.cat([targets, torch.zeros(targets.size(0), max_seq_length - targets.size(1) + 1, dtype=torch.long)], dim=1) # we add 1 to the target length to account for the shift in the decoder input\n",
    "            \n",
    "            # Adjust the slicing operation to ensure that the batch size remains the same\n",
    "            outputs = model(inputs[:, :max_seq_length], targets[:, :-1])  # Exclude the last token from targets as input to the decoder\n",
    "\n",
    "            # Reshape outputs and targets to (batch_size * seq_len, vocab_size_tgt) for loss calculation\n",
    "            outputs = outputs.view(-1, dataset.vocab_size_tgt)\n",
    "            targets = targets[:, 1:].contiguous().view(-1)  # Exclude the first token from targets for loss calculation\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            print(f\"[INFO] Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}, Running Loss: {running_loss:.4f}\")\n",
    "            print(f'-'*10)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        print(f'-'*10)\n",
    "\n",
    "    torch.save(model.state_dict(), 'transliteration_transformer.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train(model, train_loader, criterion, optimizer, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
