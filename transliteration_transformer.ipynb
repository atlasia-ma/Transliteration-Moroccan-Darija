{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 2048\n",
    "n_epochs=250\n",
    "test_size = 0.01 #(1% of the data, around 700 samples)\n",
    "\n",
    "# Transformer parameters\n",
    "d_model=512 \n",
    "nhead=8\n",
    "num_encoder_layers=4\n",
    "num_decoder_layers=4\n",
    "\n",
    "# extra parameters\n",
    "seed = 42\n",
    "logging_freq=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, DATA_HUB='atlasia/ATAM'):\n",
    "        # Load the dataset\n",
    "        self.data = load_dataset(DATA_HUB)['train'].to_pandas().values.tolist()\n",
    "        # Create a set of all unique characters in the source and target languages\n",
    "        self.arabizi_chars = set(''.join([d[0] for d in self.data]))\n",
    "        self.arabic_chars = set(''.join([d[1] for d in self.data]))\n",
    "        # Create a dictionary mapping each character to a unique index\n",
    "        self.char2idx_ary = {char: idx for idx, char in enumerate(self.arabizi_chars)}\n",
    "        self.char2idx_ar = {char: idx for idx, char in enumerate(self.arabic_chars)}\n",
    "        # Calculate the size of the vocabulary\n",
    "        self.vocab_size_src = len(self.char2idx_ary)\n",
    "        self.vocab_size_tgt = len(self.char2idx_ar)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        darija, darija_ar = self.data[idx]\n",
    "        input_seq = [self.char2idx_ary[char] for char in darija]\n",
    "        target_seq = [self.char2idx_ar[char] for char in darija_ar]\n",
    "        input = torch.LongTensor(input_seq)\n",
    "        target = torch.LongTensor(target_seq)\n",
    "        return input, target\n",
    "\n",
    "def collate_function(batch):\n",
    "    # Extract inputs and targets from the batch\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Concatenate all sequences in the batch to find the maximum length\n",
    "    all_sequences = inputs + targets\n",
    "    \n",
    "    # Find the maximum length of sequences in the batch\n",
    "    max_seq_length = max(len(seq) for seq in all_sequences)\n",
    "    \n",
    "    # Pad all sequences to the maximum length\n",
    "    padded_inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    padded_targets = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad sequences to max_seq_length\n",
    "    padded_inputs = torch.cat([padded_inputs, torch.zeros(padded_inputs.size(0), max_seq_length - padded_inputs.size(1), dtype=torch.long)], dim=1)\n",
    "    padded_targets = torch.cat([padded_targets, torch.zeros(padded_targets.size(0), max_seq_length - padded_targets.size(1), dtype=torch.long)], dim=1)\n",
    "    \n",
    "    return padded_inputs, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransliterationDataset()\n",
    "train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 66514 samples.\n",
      "The validation dataset has 672 samples.\n"
     ]
    }
   ],
   "source": [
    "print(f'The training dataset has {len(train_data)} samples.')\n",
    "print(f'The validation dataset has {len(val_data)} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationModel(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, d_model=128, nhead=2, num_encoder_layers=2, num_decoder_layers=2):\n",
    "        super(TransliterationModel, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(vocab_size_src, d_model)\n",
    "        self.embedding_tgt = nn.Embedding(vocab_size_tgt, d_model)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size_tgt)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'transformer' in name:\n",
    "                    # Initialize transformer layer weights\n",
    "                    if len(param.shape) > 1:\n",
    "                        init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    # Initialize other layer weights\n",
    "                    if 'embedding' in name:\n",
    "                        # Use Xavier initialization for embeddings\n",
    "                        init.xavier_uniform_(param)\n",
    "                    elif 'fc' in name:\n",
    "                        # Use Xavier initialization for linear layer\n",
    "                        init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding_src(src)\n",
    "        tgt = self.embedding_tgt(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 29,500,976 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = TransliterationModel(   vocab_size_src=dataset.vocab_size_src, \n",
    "                                vocab_size_tgt=dataset.vocab_size_tgt, \n",
    "                                d_model=d_model, \n",
    "                                nhead=nhead, \n",
    "                                num_encoder_layers=num_encoder_layers, \n",
    "                                num_decoder_layers=num_decoder_layers\n",
    "                             ).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, n_epochs=10, logging_freq=100, device='cuda:0'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    iteration = 0\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Ensure that inputs and targets have the same length after padding\n",
    "            max_seq_length = max(inputs.size(1), targets.size(1))\n",
    "            inputs = torch.cat([inputs, torch.zeros(inputs.size(0), max_seq_length - inputs.size(1), dtype=torch.long)], dim=1).to(device)\n",
    "            targets = torch.cat([targets, torch.zeros(targets.size(0), max_seq_length - targets.size(1) + 1, dtype=torch.long)], dim=1).to(device) # we add 1 to the target length to account for the shift in the decoder input\n",
    "            \n",
    "            # Adjust the slicing operation to ensure that the batch size remains the same\n",
    "            outputs = model(inputs[:, :max_seq_length], targets[:, :-1])  # Exclude the last token from targets as input to the decoder\n",
    "\n",
    "            # Reshape outputs and targets to (batch_size * seq_len, vocab_size_tgt) for loss calculation\n",
    "            outputs = outputs.view(-1, dataset.vocab_size_tgt)\n",
    "            targets = targets[:, 1:].contiguous().view(-1)  # Exclude the first token from targets for loss calculation\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if iteration % logging_freq == 0:\n",
    "                print(f\"[INFO-TRAIN] Epoch {epoch+1}/{n_epochs}, iteration: {iteration}, Loss: {loss.item():.4f}, Running Loss: {running_loss:.4f}\")\n",
    "                print(f'-'*10)\n",
    "                validation(model, device)\n",
    "            iteration += 1\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"[INFO-TRAIN] Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.4f}, Running Loss: {running_loss:.4f}\")\n",
    "        print(f'-'*10)\n",
    "        validation(model, device)\n",
    "\n",
    "    torch.save(model.state_dict(), 'transliteration_transformer.pth')\n",
    "    return losses, model\n",
    "\n",
    "\n",
    "def validation(model, device):\n",
    "     # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            val_inputs = val_inputs\n",
    "            val_targets = val_targets\n",
    "            \n",
    "            # Ensure that inputs and targets have the same length after padding\n",
    "            max_seq_length = max(val_inputs.size(1), val_targets.size(1))\n",
    "            val_inputs = torch.cat([val_inputs, torch.zeros(val_inputs.size(0), max_seq_length - val_inputs.size(1), dtype=torch.long)], dim=1).to(device)\n",
    "            val_targets = torch.cat([val_targets, torch.zeros(val_targets.size(0), max_seq_length - val_targets.size(1) + 1, dtype=torch.long)], dim=1).to(device) # we add 1 to the target length to account for the shift in the decoder input\n",
    "\n",
    "            val_outputs = model(val_inputs[:, :max_seq_length], val_targets[:, :-1])\n",
    "\n",
    "            val_targets = val_targets[:, 1:].contiguous().view(-1)  # Exclude the first token from targets for loss calculation\n",
    "            val_outputs = val_outputs.view(-1, dataset.vocab_size_tgt)\n",
    "\n",
    "            val_loss += criterion(val_outputs, val_targets).item() * val_inputs.size(0)\n",
    "\n",
    "    print(f\"[INFO-VAL] Validation Loss: {val_loss:.4f}\")\n",
    "    print(f'-'*10)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO-TRAIN] Epoch 1/250, iteration: 0, Loss: 3.2244, Running Loss: 6603.6611\n",
      "----------\n",
      "[INFO-VAL] Validation Loss: 4088.3644\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/250 [00:26<1:48:41, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO-TRAIN] Epoch 1/250, Loss: 1.4402, Running Loss: 95790.5047\n",
      "----------\n",
      "[INFO-VAL] Validation Loss: 753.6988\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "losses, model = train(model, train_loader, criterion, optimizer, n_epochs, logging_freq, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
