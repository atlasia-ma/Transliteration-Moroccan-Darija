{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "batch_size = 10\n",
    "test_size = 0.01 #(1% of the data, around 700 samples)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, DATA_HUB = 'atlasia/ATAM'):\n",
    "        self.data = load_dataset(DATA_HUB)['train'].to_pandas().values.tolist()\n",
    "        self.arabizi_chars = set(''.join([d[0] for d in self.data]))\n",
    "        self.arabic_chars = set(''.join([d[1] for d in self.data]))\n",
    "        self.char2idx_ary = {char: idx for idx, char in enumerate(self.arabizi_chars)}\n",
    "        self.char2idx_ar = {char: idx for idx, char in enumerate(self.arabic_chars)}\n",
    "        self.vocab_size_input = len(self.char2idx_ary)\n",
    "        self.vocab_size_output = len(self.char2idx_ar)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        darija, darija_ar = self.data[idx]\n",
    "        input_seq = [self.char2idx_ary[char] for char in darija]\n",
    "        target_seq = [self.char2idx_ar[char] for char in darija_ar]\n",
    "        input = torch.LongTensor(input_seq)\n",
    "        target = torch.LongTensor(target_seq)\n",
    "        # print(input)\n",
    "        # print(target)\n",
    "        return input, target\n",
    "    \n",
    "def collate_function_old(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad sequences to have the same length\n",
    "    padded_inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    padded_targets = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "def collate_function(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad sequences to have the same length\n",
    "    padded_inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    # Ensure targets are padded and convert to 1D tensor\n",
    "    padded_targets = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    # Flatten the padded_targets to ensure it matches the batch size of inputs\n",
    "    padded_targets = padded_targets.view(-1)  # Reshape to 1D tensor\n",
    "    return padded_inputs, padded_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransliterationDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=seed)\n",
    "print(f'The training dataset has {len(train_data)} samples.')\n",
    "print(f'The validation dataset has {len(val_data)} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, target in train_loader:\n",
    "    print(f'input: {input.shape}')\n",
    "    print(f'target: {target.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterationModel(nn.Module):\n",
    "    def __init__(self, vocab_size_input, vocab_size_output):\n",
    "        super(TransliterationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size_input, 128)\n",
    "        self.conv1d = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "        self.fc = nn.Linear(256, vocab_size_output)\n",
    "        self.vocab_size_input = vocab_size_input\n",
    "        self.vocab_size_output = vocab_size_output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # Conv1D expects input in (batch_size, in_channels, seq_len) format\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, seq_len, in_channels)\n",
    "        x = self.conv1d(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.max_pool1d(x, kernel_size=x.size(2))  # Global max pooling\n",
    "        x = x.squeeze(2)  # Squeeze to remove the channel dimension\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.vocab_size_input)\n",
    "print(dataset.vocab_size_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransliterationModel(vocab_size_input=dataset.vocab_size_input, vocab_size_output=dataset.vocab_size_output)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             print(f'inputs[0]: {inputs[0]}')\n",
    "#             print(f'targets[0]: {targets[0]}')\n",
    "#             print(f'outputs[0]: {outputs[0]}')\n",
    "#             print('--------------------------------------')\n",
    "#             print(f'outputs: {outputs.shape}')\n",
    "#             print(f'targets: {targets.shape}')\n",
    "#             print('--------------------------------------')\n",
    "#             # Convert targets to one-hot encoding\n",
    "#             targets_one_hot = F.one_hot(targets, num_classes=model.vocab_size).float()\n",
    "#             print(f'outputs: {outputs.shape}')\n",
    "#             print(f'targets: {targets_one_hot.shape}')\n",
    "#             print('--------------------------------------')\n",
    "#             print(f'outputs: {outputs}')\n",
    "#             print(f'targets: {targets_one_hot}')\n",
    "#             print('--------------------------------------')\n",
    "#             loss = criterion(outputs, targets_one_hot)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # Reshape the targets to match the shape of the outputs\n",
    "            targets = targets.view(outputs.shape[0] * outputs.shape[1])\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Evaluation on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                # Reshape the targets to match the shape of the outputs\n",
    "                targets = targets.view(outputs.shape[0] * outputs.shape[1])\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
